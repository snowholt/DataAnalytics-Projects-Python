{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21d61623-9c1b-4c1d-bcbe-02e66a935875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:27:45.222222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-30 11:27:45.223565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-30 11:27:45.224137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-30 11:27:45.224890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-30 11:27:45.225472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-30 11:27:45.225887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 2483 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16417088075855329816\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2603876352\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 15629598123103945116\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "#import torch.nn as nn\n",
    "\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import urllib\n",
    "import csv\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b85f7c-342b-468e-b563-d727f431ccf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "024910d8-c171-4d46-a57d-b5fd39ca2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-30 10:12:11--  https://dem-primary-tweets.s3.amazonaws.com/PeteForAmerica.1574004110.txt\n",
      "Resolving dem-primary-tweets.s3.amazonaws.com (dem-primary-tweets.s3.amazonaws.com)... 52.216.60.249, 52.217.100.60, 52.217.204.49, ...\n",
      "Connecting to dem-primary-tweets.s3.amazonaws.com (dem-primary-tweets.s3.amazonaws.com)|52.216.60.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15936465 (15M) [text/plain]\n",
      "Saving to: ‘PeteForAmerica.1574004110.txt’\n",
      "\n",
      "PeteForAmerica.1574 100%[===================>]  15.20M  7.64MB/s    in 2.0s    \n",
      "\n",
      "2023-05-30 10:12:14 (7.64 MB/s) - ‘PeteForAmerica.1574004110.txt’ saved [15936465/15936465]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dem-primary-tweets.s3.amazonaws.com/PeteForAmerica.1574004110.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fdd69c-b410-4361-bf02-8f0f45412cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>full_text</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-17 15:15:34+00:00</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @PeteButtigieg: I am not just here to end t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1196084469637234688</td>\n",
       "      <td>...</td>\n",
       "      <td>{'contributors': None, 'coordinates': None, 'c...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-17 15:00:00+00:00</td>\n",
       "      <td>[0, 98]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [{'dis...</td>\n",
       "      <td>211</td>\n",
       "      <td>False</td>\n",
       "      <td>“The president has again dishonored our armed ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1196080551666409473</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-17 02:00:00+00:00</td>\n",
       "      <td>[0, 14]</td>\n",
       "      <td>{'hashtags': [], 'media': [{'display_url': 'pi...</td>\n",
       "      <td>5168</td>\n",
       "      <td>False</td>\n",
       "      <td>Win with hope. https://t.co/JI1ifDCg5Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1195884258017075205</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'media': [{'display_url': 'pic.twitter.com/JI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-17 00:45:55+00:00</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [{'indices': [77, 88], 'text': 'R...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @UniNoticias: El turno de @PeteButtigieg: a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1195865616533639170</td>\n",
       "      <td>...</td>\n",
       "      <td>{'contributors': None, 'coordinates': None, 'c...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-17 00:37:15+00:00</td>\n",
       "      <td>[0, 138]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [{'dis...</td>\n",
       "      <td>258</td>\n",
       "      <td>False</td>\n",
       "      <td>Don’t miss @PeteButtigieg speak about his visi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1195863434119843842</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-03 15:25:27+00:00</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @Chas10Buttigieg: As we begin to open 20 ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168907865534468096</td>\n",
       "      <td>...</td>\n",
       "      <td>{'contributors': None, 'coordinates': None, 'c...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.168851e+18</td>\n",
       "      <td>1.168851e+18</td>\n",
       "      <td>{'display': 'twitter.com/PeteForAmerica…', 'ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-03 15:15:00+00:00</td>\n",
       "      <td>[0, 182]</td>\n",
       "      <td>{'hashtags': [{'indices': [4, 13], 'text': 'Pe...</td>\n",
       "      <td>313</td>\n",
       "      <td>False</td>\n",
       "      <td>The #PeteWave is coming to New Hampshire! Join...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168905236737646597</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2334</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-03 14:18:41+00:00</td>\n",
       "      <td>[0, 74]</td>\n",
       "      <td>{'hashtags': [{'indices': [10, 21], 'text': 'P...</td>\n",
       "      <td>1150</td>\n",
       "      <td>False</td>\n",
       "      <td>Hyped for #PhaseThree?\\n\\nBecome a volunteer t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168891064314408961</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'media': [{'additional_media_info': {'monetiz...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-03 12:46:48+00:00</td>\n",
       "      <td>[0, 140]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'urls': [], 'u...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>RT @PeteButtigieg: 20 new offices in Iowa, 12 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168867942299312131</td>\n",
       "      <td>...</td>\n",
       "      <td>{'contributors': None, 'coordinates': None, 'c...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.168851e+18</td>\n",
       "      <td>1.168851e+18</td>\n",
       "      <td>{'display': 'twitter.com/PeteForAmerica…', 'ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-03 11:38:22+00:00</td>\n",
       "      <td>[0, 236]</td>\n",
       "      <td>{'hashtags': [{'indices': [201, 212], 'text': ...</td>\n",
       "      <td>3694</td>\n",
       "      <td>False</td>\n",
       "      <td>Hello from South Bend! Welcome to the new, off...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168850719887306755</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'can_media_tag': True, 'contributors_enabled'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2337 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contributors  coordinates                created_at display_text_range  \\\n",
       "0              NaN          NaN 2019-11-17 15:15:34+00:00           [0, 140]   \n",
       "1              NaN          NaN 2019-11-17 15:00:00+00:00            [0, 98]   \n",
       "2              NaN          NaN 2019-11-17 02:00:00+00:00            [0, 14]   \n",
       "3              NaN          NaN 2019-11-17 00:45:55+00:00           [0, 140]   \n",
       "4              NaN          NaN 2019-11-17 00:37:15+00:00           [0, 138]   \n",
       "...            ...          ...                       ...                ...   \n",
       "2332           NaN          NaN 2019-09-03 15:25:27+00:00           [0, 140]   \n",
       "2333           NaN          NaN 2019-09-03 15:15:00+00:00           [0, 182]   \n",
       "2334           NaN          NaN 2019-09-03 14:18:41+00:00            [0, 74]   \n",
       "2335           NaN          NaN 2019-09-03 12:46:48+00:00           [0, 140]   \n",
       "2336           NaN          NaN 2019-09-03 11:38:22+00:00           [0, 236]   \n",
       "\n",
       "                                               entities  favorite_count  \\\n",
       "0     {'hashtags': [], 'symbols': [], 'urls': [], 'u...               0   \n",
       "1     {'hashtags': [], 'symbols': [], 'urls': [{'dis...             211   \n",
       "2     {'hashtags': [], 'media': [{'display_url': 'pi...            5168   \n",
       "3     {'hashtags': [{'indices': [77, 88], 'text': 'R...               0   \n",
       "4     {'hashtags': [], 'symbols': [], 'urls': [{'dis...             258   \n",
       "...                                                 ...             ...   \n",
       "2332  {'hashtags': [], 'symbols': [], 'urls': [], 'u...               0   \n",
       "2333  {'hashtags': [{'indices': [4, 13], 'text': 'Pe...             313   \n",
       "2334  {'hashtags': [{'indices': [10, 21], 'text': 'P...            1150   \n",
       "2335  {'hashtags': [], 'symbols': [], 'urls': [], 'u...               0   \n",
       "2336  {'hashtags': [{'indices': [201, 212], 'text': ...            3694   \n",
       "\n",
       "      favorited                                          full_text  geo  \\\n",
       "0         False  RT @PeteButtigieg: I am not just here to end t...  NaN   \n",
       "1         False  “The president has again dishonored our armed ...  NaN   \n",
       "2         False             Win with hope. https://t.co/JI1ifDCg5Z  NaN   \n",
       "3         False  RT @UniNoticias: El turno de @PeteButtigieg: a...  NaN   \n",
       "4         False  Don’t miss @PeteButtigieg speak about his visi...  NaN   \n",
       "...         ...                                                ...  ...   \n",
       "2332      False  RT @Chas10Buttigieg: As we begin to open 20 ne...  NaN   \n",
       "2333      False  The #PeteWave is coming to New Hampshire! Join...  NaN   \n",
       "2334      False  Hyped for #PhaseThree?\\n\\nBecome a volunteer t...  NaN   \n",
       "2335      False  RT @PeteButtigieg: 20 new offices in Iowa, 12 ...  NaN   \n",
       "2336      False  Hello from South Bend! Welcome to the new, off...  NaN   \n",
       "\n",
       "                       id  ...  \\\n",
       "0     1196084469637234688  ...   \n",
       "1     1196080551666409473  ...   \n",
       "2     1195884258017075205  ...   \n",
       "3     1195865616533639170  ...   \n",
       "4     1195863434119843842  ...   \n",
       "...                   ...  ...   \n",
       "2332  1168907865534468096  ...   \n",
       "2333  1168905236737646597  ...   \n",
       "2334  1168891064314408961  ...   \n",
       "2335  1168867942299312131  ...   \n",
       "2336  1168850719887306755  ...   \n",
       "\n",
       "                                       retweeted_status  \\\n",
       "0     {'contributors': None, 'coordinates': None, 'c...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3     {'contributors': None, 'coordinates': None, 'c...   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "2332  {'contributors': None, 'coordinates': None, 'c...   \n",
       "2333                                                NaN   \n",
       "2334                                                NaN   \n",
       "2335  {'contributors': None, 'coordinates': None, 'c...   \n",
       "2336                                                NaN   \n",
       "\n",
       "                                                 source  truncated  \\\n",
       "0     <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "1     <a href=\"https://about.twitter.com/products/tw...      False   \n",
       "2     <a href=\"https://about.twitter.com/products/tw...      False   \n",
       "3     <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "4     <a href=\"https://about.twitter.com/products/tw...      False   \n",
       "...                                                 ...        ...   \n",
       "2332  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "2333  <a href=\"https://about.twitter.com/products/tw...      False   \n",
       "2334  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "2335  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "2336  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "\n",
       "                                                   user  possibly_sensitive  \\\n",
       "0     {'can_media_tag': True, 'contributors_enabled'...                 NaN   \n",
       "1     {'can_media_tag': True, 'contributors_enabled'...                 0.0   \n",
       "2     {'can_media_tag': True, 'contributors_enabled'...                 0.0   \n",
       "3     {'can_media_tag': True, 'contributors_enabled'...                 0.0   \n",
       "4     {'can_media_tag': True, 'contributors_enabled'...                 0.0   \n",
       "...                                                 ...                 ...   \n",
       "2332  {'can_media_tag': True, 'contributors_enabled'...                 NaN   \n",
       "2333  {'can_media_tag': True, 'contributors_enabled'...                 0.0   \n",
       "2334  {'can_media_tag': True, 'contributors_enabled'...                 0.0   \n",
       "2335  {'can_media_tag': True, 'contributors_enabled'...                 NaN   \n",
       "2336  {'can_media_tag': True, 'contributors_enabled'...                 NaN   \n",
       "\n",
       "                                      extended_entities  quoted_status  \\\n",
       "0                                                   NaN            NaN   \n",
       "1                                                   NaN            NaN   \n",
       "2     {'media': [{'display_url': 'pic.twitter.com/JI...            NaN   \n",
       "3                                                   NaN            NaN   \n",
       "4                                                   NaN            NaN   \n",
       "...                                                 ...            ...   \n",
       "2332                                                NaN            NaN   \n",
       "2333                                                NaN            NaN   \n",
       "2334  {'media': [{'additional_media_info': {'monetiz...            NaN   \n",
       "2335                                                NaN            NaN   \n",
       "2336                                                NaN            NaN   \n",
       "\n",
       "     quoted_status_id  quoted_status_id_str  \\\n",
       "0                 NaN                   NaN   \n",
       "1                 NaN                   NaN   \n",
       "2                 NaN                   NaN   \n",
       "3                 NaN                   NaN   \n",
       "4                 NaN                   NaN   \n",
       "...               ...                   ...   \n",
       "2332     1.168851e+18          1.168851e+18   \n",
       "2333              NaN                   NaN   \n",
       "2334              NaN                   NaN   \n",
       "2335     1.168851e+18          1.168851e+18   \n",
       "2336              NaN                   NaN   \n",
       "\n",
       "                                quoted_status_permalink  \n",
       "0                                                   NaN  \n",
       "1                                                   NaN  \n",
       "2                                                   NaN  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "2332  {'display': 'twitter.com/PeteForAmerica…', 'ex...  \n",
       "2333                                                NaN  \n",
       "2334                                                NaN  \n",
       "2335  {'display': 'twitter.com/PeteForAmerica…', 'ex...  \n",
       "2336                                                NaN  \n",
       "\n",
       "[2337 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('PeteForAmerica.1574004110.txt', lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e71ddc-d6d2-4f51-a4a9-5e820b9f1a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2043e-04, 9.9978e-01],\n",
       "        [5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    " \n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pt_batch = tokenizer(\n",
    "[\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\")\n",
    " \n",
    "pt_outputs = pt_model(**pt_batch)\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=1)\n",
    "pt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2efc4ded-6cf3-409a-8d15-803f58343d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device_ids = [0,1,2,3]\n",
    "dev = 'cuda'\n",
    "if dev == 'cpu':\n",
    "  device = torch.device('cpu')\n",
    "  device_staging = 'cpu:0'\n",
    "else:\n",
    "  device = torch.device('cuda')\n",
    "  device_staging = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f635f628-c349-42ed-8cab-dd0511eb6e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# Create a list of available GPU device IDs\n",
    "device_ids = []\n",
    "for i in range(num_gpus):\n",
    "    device_ids.append(i)\n",
    "\n",
    "print(device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffe8996a-86f7-4f3b-8d12-28a1b724cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Get the device name\n",
    "device_name = torch.cuda.get_device_name('cuda:0')\n",
    "\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77fd9f6c-8de3-4b0a-9f52-ffeff22c3ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 3.82 GiB total capacity; 2.71 GiB already allocated; 33.25 MiB free; 2.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[1;32m     53\u001b[0m all_files \u001b[38;5;241m=\u001b[39m get_all_files()\n\u001b[0;32m---> 54\u001b[0m model3 \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_staging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m model3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model3)  \u001b[38;5;66;03m# Use all available GPUs\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Analytics/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Analytics/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Analytics/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/Analytics/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Analytics/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/Analytics/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 3.82 GiB total capacity; 2.71 GiB already allocated; 33.25 MiB free; 2.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "MODEL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "def get_all_files():\n",
    "    file_list = ['PeteForAmerica.1574004110.txt']\n",
    "    return file_list\n",
    "\n",
    "\n",
    "class TextLoader(Dataset):\n",
    "    def __init__(self, file=None, transform=None, target_transform=None, tokenizer=None):\n",
    "        self.file = pd.read_json(file, lines=True)\n",
    "        self.file = self.file\n",
    "        self.file = tokenizer(list(self.file['full_text']), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        self.file = self.file['input_ids']\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.file[idx]\n",
    "        return data\n",
    "\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        self.fc = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        pt_predictions = nn.functional.softmax(output.logits, dim=1)\n",
    "        return pt_predictions\n",
    "\n",
    "\n",
    "dev = 'cuda'\n",
    "if dev == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "    device_staging = 'cpu'\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    device_staging = 'cuda:0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "all_files = get_all_files()\n",
    "model3 = SentimentModel().to(device_staging)\n",
    "model3 = nn.DataParallel(model3)  # Use all available GPUs\n",
    "\n",
    "try:\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Create a GradScaler for mixed-precision training\n",
    "    torch.set_printoptions(threshold=10000)\n",
    "\n",
    "    for file in all_files:\n",
    "        data = TextLoader(file=file, tokenizer=tokenizer)\n",
    "        train_dataloader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "        out = torch.empty(0, 0).to(device_staging)\n",
    "\n",
    "        for data in train_dataloader:\n",
    "            input = data.to(device_staging)\n",
    "\n",
    "            # Enable mixed-precision training\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model3(input)\n",
    "\n",
    "            # Rest of your code...\n",
    "\n",
    "        df = pd.read_json(file, lines=True)['full_text']\n",
    "        res = out.cpu().numpy()\n",
    "        df_res = pd.DataFrame({\"text\": df, \"negative\": res[:, 0], \"positive\": res[:, 1]})\n",
    "        print(df_res)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a01e5-29aa-4efb-8fc0-5e5a03e2a0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
